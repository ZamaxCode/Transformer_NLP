{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a02071c0-044f-4ead-a98d-7aa4111cb71a",
   "metadata": {},
   "source": [
    "# Self Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67c7aad-5300-47de-8716-0dd34dc3eea5",
   "metadata": {},
   "source": [
    "### Terminos a usar:\n",
    "* Q - Query (Que estoy buscando)\n",
    "* K - Key (Que puedo ofrecer)\n",
    "* V - Value (Lo que ofresco)\n",
    "* d_X - Dimension de X conjunto (dV, dK, dQ, ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9021f5c3-4b7a-463f-a36f-8a1f0dc28fe2",
   "metadata": {},
   "source": [
    "De acuerdo con el paper de Attention is all you need, una funcion de atencion se describe como el mapeo de la Query y el conjunto de pares de Key-Values a una salida, donde Q-K-V son vectores. La salida es computada como una sumatoria con pesos de estos valores, y donde los pesos son obtenidos mediante una funcion de compatibilidad entre la query y la key correspondiente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39873937-1f8f-4ee8-af8e-9ceaa7528cb3",
   "metadata": {},
   "source": [
    "Primeramente, para abordar esta parte tenemos que generar nuestos datos Q-K-V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3b4d87f1-f709-498e-98c4-978421247773",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "phrase = \"Hola mi nombre es Alex\"\n",
    "input = phrase.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2b252ea-2481-4dfd-abac-1a4dc53dde92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hola', 'mi', 'nombre', 'es', 'Alex']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "21d50c23-0513-465f-9fe1-ed4626200b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_len = len(input)\n",
    "d_K, d_V = 8, 8\n",
    "q = np.random.randn(in_len, d_K)\n",
    "k = np.random.randn(in_len, d_K)\n",
    "v = np.random.randn(in_len, d_V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0000d2b4-333f-4dd9-a910-9df42775246b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.28992813,  0.85906143,  0.21495725, -2.1308364 ,  1.02194232,\n",
       "        -0.36578623, -0.59514708, -0.46351817],\n",
       "       [ 1.87962513, -0.02568894, -0.51524885,  1.23020388, -0.67709758,\n",
       "        -0.91531064,  0.16847735,  0.2347544 ],\n",
       "       [-0.2302133 , -0.51736049,  0.22206796, -0.21667557,  0.98000167,\n",
       "        -0.25227682,  1.05231845,  0.03994449],\n",
       "       [-0.57217994, -0.13183049, -0.7318054 , -0.90349994,  0.4996766 ,\n",
       "         0.51383088,  1.47310469, -0.82662214],\n",
       "       [ 0.71240792,  0.60729948,  1.27335944,  0.31328028, -1.56159438,\n",
       "         0.12807969, -0.73653941,  1.42056491]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "174bd7b6-7137-4ddc-a705-26a4835ea66e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.58231432, -0.19734611, -1.00242319, -0.03805725, -1.28308434,\n",
       "         0.8047156 ,  0.89423467,  0.17954249],\n",
       "       [ 0.36907596, -1.14338458, -0.93052493, -0.5720036 ,  0.65796214,\n",
       "         0.10875771,  0.39224373, -0.1506256 ],\n",
       "       [-0.09344037,  0.80946902,  0.72635436,  0.34858423,  0.60613819,\n",
       "        -3.30908328, -1.47275347,  0.1766531 ],\n",
       "       [-1.61318901,  0.30499548,  1.20877345,  0.9539165 ,  1.00297772,\n",
       "         1.20746745,  2.98391289, -0.11909048],\n",
       "       [-1.08699605,  0.60438553,  0.6649686 , -0.12199497, -0.52800097,\n",
       "        -0.27451453, -0.15154801,  0.91716749]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eaff9369-ba38-433b-bbd0-ae54f0fd882e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.09988025,  0.01087252, -0.66951067, -1.98764867,  1.22440004,\n",
       "         0.12023061, -0.35682981, -0.18709454],\n",
       "       [-0.05035914, -0.86577554,  2.00003202, -1.35962342,  0.48887871,\n",
       "         0.11802904, -0.21923461,  1.30231047],\n",
       "       [-0.24799169,  0.22273024,  0.77994725, -0.55120025,  1.04939514,\n",
       "        -0.61955624, -2.75513078, -1.22062181],\n",
       "       [ 0.09765184, -0.98867081, -0.83415428, -0.02786349,  0.20555664,\n",
       "         0.42917456,  0.29869629, -0.32787272],\n",
       "       [ 0.17731818,  0.85586311, -0.86626718, -1.6316923 ,  1.91028236,\n",
       "        -0.93733119, -1.97092547, -1.42572689]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531795f8-d2de-4886-988f-536b8ace0566",
   "metadata": {},
   "source": [
    "Una vez teniendo estos, vamos a ser capaces de representar la parte de la funcion de atencion o el scaled dot-product attention, tomando como base lo que hay el en paper.\n",
    "\n",
    "<img src=\"./images/scaled_dot_prod_attent.png\" alt=\"scaled_dot_prod_attent\" width=\"200\" height=\"auto\"> <img src=\"./images/attention.png\" alt=\"attention formula\" width=\"400\" height=\"auto\">\n",
    "\n",
    "Para esto, en la formula mostrada en el paper se ignora la parte de la Mask, ya que esta es opcional, pero bastante util. unicamente se a√±ade a la formula sumandola al resultado de lo que la division."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c741066d-91b2-4563-87b9-9bbd34e191d9",
   "metadata": {},
   "source": [
    "Funcion softmax se representa como: \n",
    "\n",
    "<img src=\"./images/softmax.png\" alt=\"softmax func\" width=\"200\" height=\"auto\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "03ed34c2-3fb4-49fc-8dbb-b799100b7149",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return (np.exp(x).T / np.sum(np.exp(x), axis=-1)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fdef8f92-5036-45cc-8847-7595c33b0f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled = np.matmul(q, k.T)/math.sqrt(d_K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6707e3df-4d86-45f9-beba-58a6c43bc712",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mask optional\n",
    "mask = np.tril(np.ones( (in_len, in_len) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7c5b71a1-3df2-485f-bede-a3d05b54a958",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0.],\n",
       "       [1., 1., 0., 0., 0.],\n",
       "       [1., 1., 1., 0., 0.],\n",
       "       [1., 1., 1., 1., 0.],\n",
       "       [1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1dd07bdf-35b2-4b78-ba84-bf2a3da3e5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask[mask == 0] = -np.infty\n",
    "mask[mask == 1] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e33381f5-f55d-4dc3-801f-d0701f36bc34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0., -inf, -inf, -inf, -inf],\n",
       "       [  0.,   0., -inf, -inf, -inf],\n",
       "       [  0.,   0.,   0., -inf, -inf],\n",
       "       [  0.,   0.,   0.,   0., -inf],\n",
       "       [  0.,   0.,   0.,   0.,   0.]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f30bba67-ea6f-435c-8586-472d3a090861",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention = softmax(scaled+mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2ae92d9f-a521-4bec-88c9-90c2c0147465",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.31793058, 0.68206942, 0.        , 0.        , 0.        ],\n",
       "       [0.26506949, 0.48489931, 0.2500312 , 0.        , 0.        ],\n",
       "       [0.24693096, 0.21313512, 0.01900443, 0.52092948, 0.        ],\n",
       "       [0.13352946, 0.05557004, 0.29655043, 0.06418016, 0.45016991]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cd839a2f-33c7-4933-95ec-9ede2475c28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = np.matmul(attention, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1c98f34d-5d01-4c7b-8c02-580714d0618b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.58386373, -0.38535245,  0.21646513,  0.49997813,  1.14483231,\n",
       "         0.35961925, -0.6517341 , -0.59169241],\n",
       "       [-0.29159001,  0.49177799,  0.77658553,  1.39734939,  0.59765795,\n",
       "        -0.1503909 , -0.30023651, -0.76811127],\n",
       "       [-0.22086216, -0.11948308,  0.33831905,  0.79815492,  0.53506018,\n",
       "         0.01258004, -0.70790177, -0.80850755],\n",
       "       [ 0.22481361,  0.90139912,  0.35280004,  1.58500633,  0.74560973,\n",
       "         0.37851397, -1.21530399,  0.45751405],\n",
       "       [-0.12599258, -0.3066491 ,  0.19512373,  0.90484334,  0.26112036,\n",
       "         0.48219163, -0.52767298,  0.06996178]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76500843-44c4-448f-9c27-116c5d5788f2",
   "metadata": {},
   "source": [
    "Todo esto conforma la parte del Scaled Dot Product Attention. El siguiente paso es escalar eso al muli-head attention, que en escencia es lo mismo pero a mayor escala.\n",
    "\n",
    "<img src=\"./images/multi_head_attention.png\" alt=\"scaled_dot_prod_attent\" width=\"300\" height=\"auto\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0815fef6-da99-4b7f-8fd2-07a274efca94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
